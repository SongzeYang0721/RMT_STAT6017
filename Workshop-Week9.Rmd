---
title: "Workshop - Week 9"
output:
  html_document:
    fig_height: 4
    toc: yes
---

```{r}
options(digits=4)
```

In this workshop we are going to look at multivariate regression. One of our references is the book by Johnson and Wichern (2007) - Chapter 7. Some of the aims of this workshop are:

- Do multivariate regression using `lm`
- Compare to 'hand calculated' version
- Think about some of the results in the lecture

# Multivariate Regression

The multivariate linear regression model is (see Section 7.7 in Johnson and Wichern and lectures) given by
$$
\mathbf{Y} = \mathbf{X} \beta + \varepsilon
$$
where $\mathbf{Y}$ is of size $n \times p$, $\mathbb{X}$ is of size $n \times (q+1)$, $\beta$ is of size $(q+1) \times p$, and $\varepsilon$ is of size $n \times p$. The errors $\varepsilon = (\varepsilon_{ij})$ have zero mean and $\text{Cov}(\varepsilon_{(i)}, \varepsilon_{(k)}) = \sigma_{ij} I$

## Bivariate case (p=2)

We are first going to look at the the bivariate case ($p=2$) for a simple data set that is used in Johnson and Wichern (2007). This data come from exercise 7.25 and involve n=17 overdoses of the drug Amitriptyline (Rudorfer, 1982).

There are two responses we want to model: 
 - TOT 
 - AMI
 
Here, TOT is total TCAD plasma level and AMI is the amount of Amitriptyline present in the TCAD plasma level.

The covariates are as follows:
 - GEN, gender (male = 0, female = 1)
 - AMT, amount of drug taken at time of overdose
 - PR, PR wave measurement
 - DIAP, diastolic blood pressure
 - QRS, QRS wave measurement

We download the data from a public website:
```{r}
ami_data = read.table("http://static.lib.virginia.edu/statlab/materials/data/ami_data.DAT")
names(ami_data) <- c("TOT","AMI","GEN","AMT","PR","DIAP","QRS")
```

```{r}
ami_data
```

We look at a summary of the data:
```{r}
summary(ami_data)
```

And the dimensions:
```{r}
dim(ami_data)
```

We can look at a pairs plot to view how the various covariates vary with respect to each other.
```{r}
pairs(ami_data, col="darkmagenta", pch=18)
```

The inbuilt `lm` regression command can handle multivariate regression models. We fit a model with the response (TOT, AMI) and print a summary of the model.

We shall see that it fits a separate model for each of the responses.

```{r}
class(ami_data)
```

```{r}
colnames(ami_data)
```

```{r}
mlm1 = lm(cbind(TOT, AMI) ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
mlm1
```


```{r}
summary(mlm1)
```

It basically gives the same output as fitting the models separately:
```{r}
m1 = lm(TOT ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(m1)
m2 <- lm(AMI ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(m2)
```

We can extract the estimated $\hat\beta$ of the fitted model.
```{r}
beta.hat = coef(mlm1)
beta.hat
```
Remember, $\beta$ is of size $(q+1) \times p$, so we can check:
```{r}
dim(beta.hat)
```

We could find $\hat\beta$ directly ourselves. First, we extract the appropriate columns from the `data.frame` to define $\mathbb{X}$ and $\mathbb{Y}$.

```{r}
X = model.matrix(~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
X
```

```{r}
Y = model.matrix(~ 0 + TOT + AMI, data = ami_data)
Y
```

Then $\hat \beta$ is given by (page 3 in lecture):
```{r}
beta.hat.byhand = solve(t(X) %*% X) %*% t(X) %*% Y
beta.hat.byhand
```
Notice that our "by hand" estimate matches the one computed using the in-built `lm` function.

```{r}
beta.hat
```

Sum of squares and products could be calculated as:

```{r}
t(Y) %*% Y - t(beta.hat)%*% t(X) %*% X %*% beta.hat
```

or as:

```{r}
t(Y-X%*%beta.hat) %*% (Y-X%*%beta.hat)
```

or as the residual standard error (RSE) $s(\hat \beta)$, see page 2 lecture, given by taking the trace. The trace can be calculated by summing the diagonal entries of the matrix.

```{r}
tr = function(x) sum(diag(x))
```

```{r}
tr( t(Y-X%*%beta.hat) %*% (Y-X%*%beta.hat) )
```

```{r}
dim(ami_data)
```

Under a normality assumptions, the maximum likelihood estimator (MLE) for $\hat \Sigma$ is given by (see page 3 of lecture):
```{r}
n = dim(ami_data)[1]
Sigma.hat = t(Y-X%*%beta.hat) %*% (Y-X%*%beta.hat) / n
Sigma.hat
```

```{r}
dim(beta.hat)
```

# Low-rank inference

We want to test that the matrix $\beta$ has a low rank structure, that is, a subset of the features/covariates play no role in predicting the targets/responses (i.e., TOT and AMI).

The standard approach is to perform hypothesis testing (see page 5 in lecture). We want to partition $\beta = [\beta_1 \,\, \beta_2]$ and then test the hypothesis that
\[ H_0 : \beta_1 = 0. \]

The library `car` can handle multivariate regression and perform hypothesis testing on covariates being zero. Suppose we want to test the hypothesis that the coefficients of $\beta$ associated with the covariates PR, DIAP, and QRS are all zero. We could do:

```{r}
library(car)
```

```{r}
mlm1
```

```{r}
lh.out = linearHypothesis(mlm1, hypothesis.matrix = c("PR = 0", "DIAP = 0", "QRS = 0"), test="Wilks")
lh.out
```
Notice that the "Sum of squares and products for error" (ie. residual) is the same as what we calculated above. It is $n \hat \Sigma = n \hat \Sigma_\Omega$ for the full model $(\Omega)$. This was $G$ in the lecture (e.g., see page 12 in lecture):

```{r}
G = n * Sigma.hat
G
```

We can compute the reduced model $(\omega)$ by removing the covariates:
```{r}
Xr = model.matrix(~ GEN + AMT, data = ami_data)
Yr = model.matrix(~ 0 + TOT + AMI, data = ami_data)
```

```{r}
colnames(ami_data)
```

```{r}
Xr
```

```{r}
beta.hat.reduced = solve(t(Xr) %*% Xr) %*% t(Xr) %*% Yr
beta.hat.reduced
```

We can calculate $\hat \Sigma_1 = \hat \Sigma_\omega$ the "sum of squares for error" for the reduced model ($\omega$).
```{r}
Sigma1.hat = t(Yr-Xr%*%beta.hat.reduced) %*% (Yr-Xr%*%beta.hat.reduced) / n
Sigma1.hat
```

The "extra sum of squares" is given by the difference. This is called the "Sum of squares and products for the hypothesis" in the model output above. Notice we get the same result. This is the same $H$ as in the lecture page 11.
```{r}
H = n * Sigma1.hat - n * Sigma.hat
H
```

The values of G and H can actually be extracted from the model above.
```{r}
G = lh.out$SSPE
H = lh.out$SSPH
```

The Wilks test is what we studied in the lecture. The Wilks test statistic (i.e., likelihood ratio test) can be calculated "by hand" as:
```{r}
det(G)/det(G + H)
```

```{r}
linearHypothesis(mlm1, hypothesis.matrix = c("PR = 0", "DIAP = 0", "QRS = 0"), test = "Wilks", verbose=TRUE)
```

```{r}
row.names(coef(mlm1))
```

```{r}
L = makeHypothesis(row.names(coef(mlm1)), c("PR = 0", "DIAP = 0", "QRS = 0"))
rhs = L[, NCOL(L)]
L = L[, -NCOL(L), drop = FALSE]
```

Estimated linear function (hypothesis.matrix %*% coef - rhs):
```{r}
value.hyp = L %*% beta.hat - rhs
value.hyp
```

Sum of squares and products for the hypothesis:
```{r}
H
```

```{r}
q1 = NROW(L)
q1
```

```{r}
df = df.residual(mlm1)
df + q1
```

```{r}
pf(H/q1, q1, df, lower.tail = FALSE)
```

We accept the null hypothesis ("PR = 0", "DIAP = 0", "QRS = 0") and remove these covariates from the model:
```{r}
mlm2 = update(mlm1, . ~ . - PR - DIAP - QRS)
summary(mlm2)
```

We can add new data and predict the response using:
```{r}
nd = data.frame(GEN = 1, AMT = 1200)
p = predict(mlm2, nd)
```

And print out the prediction:
```{r}
p
```

```{r}
predictionEllipse <- function(mod, newdata, level = 0.95){
  lev_lbl <- paste0(level * 100, "%")
  resps <- colnames(mod$coefficients)
  title <- paste(lev_lbl, "confidence ellipse for", resps[1], "and", resps[2])
  
  # prediction
  p <- predict(mod, newdata)
  
  # center of ellipse
  cent <- c(p[1,1],p[1,2])
  
  # shape of ellipse
  Z <- model.matrix(mod)
  Y <- mod$model[[1]]
  n <- nrow(Y)
  m <- ncol(Y)
  r <- ncol(Z) - 1
  S <- crossprod(resid(mod))/(n-r-1)
  
  # radius of circle generating the ellipse
  tt <- terms(mod)
  Terms <- delete.response(tt)
  mf <- model.frame(Terms, newdata, na.action = na.pass, xlev = mod$xlevels)
  z0 <- model.matrix(Terms, mf, contrasts.arg = mod$contrasts)
  rad <- sqrt((m*(n-r-1)/(n-r-m))*qf(level,m,n-r-m)*z0%*%solve(t(Z)%*%Z) %*% t(z0))
  
  # generate ellipse using ellipse function in car package
  ell_points <- car::ellipse(center = c(cent), shape = S, radius = c(rad), draw = FALSE)
  
  plot(ell_points, type = "l", xlab = resps[1], ylab = resps[2], main = title)
  points(x = cent[1], y = cent[2])
}
```


```{r}
predictionEllipse(mlm2, nd)
```

# Multivariate regression (p > 2)

```{r}
library(mvtnorm)
```

We are going to generate some fake data by making a mean vector and a covariance matrix.
```{r}
n = 100
q = 5
p = 5

mu = runif(q, 1, 11)

U = matrix(runif(q*q, 0, 1), q, q)
Sigma = (t(U) + U)/2
```

```{r}
Sigma
```
```{r}
dim(Sigma)
```

Let's check that $\Sigma$ is positive definite:
```{r}
lambda = min(eigen(Sigma, only.values = TRUE)$values)
lambda
```

If $\lambda < 0$ then the matrix $\Sigma$ is not positive definite, so we must fix it. One approach is to add values down the diagonal.

```{r}
Sigma = Sigma + (1+1e-10) * abs(lambda) * diag(1, p, p)
```

```{r}
Sigma
```
We check again:
```{r}
min(eigen(Sigma, only.values = TRUE)$values)
```

```{r}
cov2cor(Sigma)
```

Alternatively, we could use the eigen decomposition
```{r}
U = matrix(runif(q*q, 0, 1), q, q)
Sigma = (t(U) + U)/2

E = eigen(Sigma)
V = E$vectors
D = E$values
```

We have some negative eigenvalues:
```{r}
D
```

So we replace them with some different random ones and then reconstruct $\Sigma$.
```{r}
D = runif(q, 0, max(D))
Sigma = V %*% diag(D) %*% t(V)
```

```{r}
Sigma
```
```{r}
min(eigen(Sigma, only.values = TRUE)$values)
```

```{r}
cov2cor(Sigma)
```

```{r}
X = data.frame(rmvnorm(n, mu, sigma=Sigma))
names(X) = paste0("c", seq(q))
```

```{r}
X
```

```{r}
w = rbinom(q, 5, 0.2)
w
```

```{r}
Ys = c()
for (j in 1:p) {
  w = rbinom(q, 5, 0.2)  
  Y = w[1] * X[,1]
  for (i in 2:q) {
    Y = Y + w[i] * X[,i]
  }
  Y = Y + rnorm(p)
  Ys = cbind(Ys, Y)
}
Ys = data.frame(Ys)
names(Ys) = paste0("r", seq(p))
```

```{r}
dim(Ys)
```

```{r}
Ys
```
We are going to create a formula for the model:
```{r}
RHS = paste(paste0("c", 1:q), collapse= "+")
RHS
```

```{r}
LHS = paste("cbind(", paste0("r", 1:p, collapse=",") ,")")
LHS
```

```{r}
fmla <- as.formula(paste(LHS, " ~ ", RHS))
fmla
```

```{r}
data = data.frame(cbind(Ys, X))
data
```

We define the multivariate regression model. The inbuilt function `lm()` actually handles multivariate models.
```{r}
model <- lm(fmla, data = data)
summary(model)
```

```{r}
X = model.matrix(as.formula(paste("~", RHS)), data = data)
names(X) = paste0("c", seq(q))
```

```{r}
X
```

```{r}
Y = model.matrix(as.formula(paste("~ 0 + ", LHS)), data = data)
```

```{r}
Y
```

Then $\hat \beta$ is given by:
```{r}
beta.hat = solve(t(X) %*% X) %*% t(X) %*% Y
beta.hat = unname(zapsmall(beta.hat))
beta.hat
```
```{r}
model
```

```{r}
linearHypothesis(model, hypothesis.matrix = c("c5 = 0"), test="Wilks")
```

