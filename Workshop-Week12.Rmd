---
title: "Workshop - Week 12"
---

```{r}
library(mvnfast)
```

In the first part, we are going to remind some basic results around the Hotelling $T^2$ test statistic that was explored in Tutorial 4 and Tutorial 5.

# Testing means: a classic result


## Checking the theorem - Anderson (2003), Theorem 5.2.2

```{r}
p = 10
n = 100
nsims = 500

Sigma = diag(p)
nu = rep(1, p)

test.stat = replicate(nsims, {
  y = t( rmvn(1, nu, Sigma) )
  S = rWishart(1, n, Sigma)[,,1] / n
  T.sq = t(y) %*% solve(S) %*% y
  (n-p+1)/(n*p) * T.sq
})

ncp = t(nu) %*% solve(Sigma) %*% nu

hist(test.stat, "FD", freq=FALSE)
curve(df(x, df1=p, df2=(n-p+1), ncp=ncp), 0, 1.2*max(test.stat), 
      col="darkgreen", lwd=2, add=TRUE)
```


## Testing that the mean is zero

The Hotelling $T^2$ test statistic is given by
$$
T^2 = n\,(\bar{\mathbf{x}} - \mu_0 )^T \mathbb{S}^{-1} (\bar{\mathbf{x}} - \mu_0 )
$$
and if we assume that the observations $\mathbf{x}_1, \ldots, \mathbf{x}_n$ come from a multivariate Normal distribution then $T^2$ is distributed like a $\chi^2$ and we can use that knowledge to construct a hypothesis test:
$$
H_0: \mu = \mu_0
$$
By default we assume that $\mu_0$ is the zero vector. 

### Implementation

We can implement the one-sample hypothesis testing algorithm as:

```{r}
hotelling = function(x, mu0=rep(0,ncol(x))) {
  n = nrow(x)
  p = ncol(x)
  #stopifnot(n >= 5 * p)
  
  bar.x = colMeans(x)
  S = cov(x)
  S.inv = solve(S)
  
  T2 = n * t(bar.x - mu0) %*% S.inv %*% (bar.x - mu0)
  
  p.value = pchisq(q = T2, df = p, lower.tail = FALSE)

  return(list(stat=T2, pvalue=p.value))
}
```

### Testing the algorithm

We can test this by generating some test data.

```{r}
n = 100
p = 3

mu = rep(0, p)
Sigma = 0.3^abs(outer(1:p, 1:p, "-")) # AR1 covariance

X = rmvn(n, mu, Sigma)
```

```{r}
dim(X)
```

```{r}
colMeans(X)
```

```{r}
hotelling(X)
```

### Compare to another package

You can compare the result to the implementation from the `ICSNP` package by installing the package first with `install.packages('ICSNP')`

```{r}
library(ICSNP)
```

```{r}
HotellingsT2(X, mu=mu, test = 'chi')
```

```{r}
HotellingsT2(X, mu=mu, test = 'f')
```

## Understanding the performance 

Recall that if you are testing a hypothesis concerning a parameter lying in a parameter space $\Theta$ of the form
$$
H_0: \theta \in \Theta_0 \quad\text{vs.}\quad H_1: \theta \in \Theta_1
$$
where $\Theta = \Theta_0 \cup \Theta_1$. Two types of error can occur:

  - Type I Error: the null hypothesis is rejected when in fact the null hypothesis is true
  
  - Type II Error: the null hypothesis is not rejected when in fact the null hypothesis is false

### Empirical Size of the Test (Prob. of Type I Error)

We can empirically study the Type I errors using a Monte-Carlo simulation.

This is to evaluate whether the 'size' of the test achieves the advertised $\alpha$. To do this, we generate data with $\theta \in \Theta_0$ and then calculate the proportion of rejections of the null hypothesis $\theta \in \Theta_0$. In our case,
\[ H_0: \theta \in \Theta_0 
\quad \Leftrightarrow \quad H_0: \mu = \mu_0  \text{ where } \mu_0 = (0,0, \ldots, 0)^T.\]

We should expect that the proportion is $\approx \alpha$ for our choice of $\alpha = 0.05$.

```{r}
M = 10000 # MC replicates

alpha = 0.05

n = 100
mu = rep(0, p)
Sigma = 0.3^abs(outer(1:p, 1:p, "-")) # AR1 covariance

p.values = numeric(M)
for (j in 1:M) {
  X = rmvn(n, mu, Sigma)
  h = hotelling(X, mu)
  p.values[j] = h$pvalue
}

p.hat = mean(p.values < alpha) # proportion of observed Type I errors

se.hat = sqrt(p.hat*(1 - p.hat)/M)

c(p.hat, se.hat)
```

We see that the empirical Type I error rate is `p.hat` and the standard error of the estimate is approximately given by `se.hat`. Ideally the error rate should be approximately equal to $\alpha$. For $p=3$, this seems to be working well.


### Higher dimensionality

```{r}
M = 1000 # MC replicates

alpha = 0.05
p = 20

n = 100
mu = rep(0, p)
Sigma = 0.3^abs(outer(1:p, 1:p, "-")) # AR1 covariance

p.values = numeric(M)
for (j in 1:M) {
  X = rmvn(n, mu, Sigma)
  h = hotelling(X, mu)
  p.values[j] = h$pvalue
}

p.hat = mean(p.values < alpha) # proportion of observed Type I errors

se.hat = sqrt(p.hat*(1 - p.hat)/M)

c(p.hat, se.hat)
```

We see that as $p$ increases the size is no longer $\approx \alpha$ as it should be!

### Empirical Power of the Test (i.e. Prob. of Type II Error)

The probability of a Type II error is $1-\mathbb{P}(\text{reject } H_0 | H_1 \text{ is true})$.

To evaluate power of a test using simulation, we generate data that is in the $H_1: \theta \in \Theta_1$ regime and calculate the proportion of rejections of $H_0$. In our case,
\[ H_1: \theta \in \Theta_1 
\quad \Leftrightarrow \quad H_1: \mu \ne \mu_0.\]

We first perform the simulation for a low value of $p$. We take $\mu_0 = (5,5)$ and we vary $\mu_1$ to understand the power as $\mu_1$ changes.

```{r}
M = 1000 # MC replicates

n = 100
p = 2

rho = 0.1
Sigma = rho^abs(outer(1:p, 1:p, "-")) # AR1 covariance

alpha = 0.05

mu0 = rep(5, p)
mu1s = seq(4.95, 5.4, length.out = 20) # alternatives
m = length(mu1s)

sim = numeric(m)
for (i in 1:m) {
  p.values = numeric(M)
  for (j in 1:M) {
    mu1 = rep(mu1s[i], p)
    X = rmvn(n, mu1, Sigma)
    h = hotelling(X, mu0)
    p.values[j] = h$pvalue
  }
  sim[i] = mean(p.values <= alpha)
}

power.low = sim
```

Now we perform the simulation for a higher value of $p$.

```{r}
M = 1000 # MC replicates

n = 100
p = 10
rho = 0.1
Sigma = rho^abs(outer(1:p, 1:p, "-")) # AR1 covariance

alpha = 0.05

mu0 = rep(5, p)
mu1s = seq(4.95, 5.4, length.out = 20) # alternatives
m = length(mu1s)

sim = numeric(m)
for (i in 1:m) {
  p.values = numeric(M)
  for (j in 1:M) {
    mu1 = rep(mu1s[i], p)
    X = rmvn(n, mu1, Sigma)
    h = hotelling(X, mu0)
    p.values[j] = h$pvalue
  }
  sim[i] = mean(p.values <= alpha)
}

power.high = sim
```

```{r}
plot(mu1s, power.low, ylim=c(0,1), type='l', col='darkgreen', lwd=2, ylab='Power (1-pi)', xlab='mu1')
lines(mu1s, power.high, col='darkmagenta', lwd=2)
abline(v = mu0, lty=2)
abline(h = alpha, lty=2)
title("Empirical power")
```

More formally, the power of a test is the power function $\pi: \Theta \to [0,1]$ which is the probability $\pi(\theta)$ of rejecting $H_0$ given that the true value of the parameter is $\theta$. Thus, for a given $\theta \in \Theta_1$, the probability of Type II error is $1-\pi(\theta_1)$. As $\mu_1$ increases the probability of Type II error is $1 - \pi(\theta_1)$, the higher the dimension the more likely we are to get a Type II error ($H_0$ is not rejected when in fact $H_0$ is false).

```{r}
plot(mu1s, 1-power.low, ylim=c(0,1), type='l', col='darkgreen', ylab='', xlab='mu1', lwd=2)
lines(mu1s, 1-power.high, col='darkmagenta', lwd=2)
abline(v = mu0, lty=2)
abline(h = alpha, lty=2)
title("Probability of Type II error")
```

### Power against sample size

We could also consider power against sample size for $p=2$ and $p=10$.

```{r}
M = 1000 # MC replicates

p = 2
rho = 0.1
Sigma = rho^abs(outer(1:p, 1:p, "-")) # AR1 covariance

alpha = 0.05

mu0 = rep(5, p)
mu1 = rep(5.1, p)

ns = seq(50, 1000, length.out = 20) # sample sizes
m = length(ns)

sim = numeric(m)
for (i in 1:m) {
  p.values = numeric(M)
  for (j in 1:M) {
    X = rmvn(as.integer(ns[i]), mu1, Sigma)
    h = hotelling(X, mu0)
    p.values[j] = h$pvalue
  }
  sim[i] = mean(p.values <= alpha)
}

power.samplesize.low = sim
```

```{r}
M = 1000 # MC replicates

p = 10
rho = 0.1
Sigma = rho^abs(outer(1:p, 1:p, "-")) # AR1 covariance

alpha = 0.05

mu0 = rep(5, p)
mu1 = rep(5.1, p)

ns = seq(50, 1000, length.out = 20) # sample sizes
m = length(ns)

sim = numeric(m)
for (i in 1:m) {
  p.values = numeric(M)
  for (j in 1:M) {
    X = rmvn(as.integer(ns[i]), mu1, Sigma)
    h = hotelling(X, mu0)
    p.values[j] = h$pvalue
  }
  sim[i] = mean(p.values <= alpha)
}

power.samplesize.high = sim
```

```{r}
plot(ns, power.samplesize.low, ylim=c(0,1), type='l', col='darkgreen', ylab='Power', xlab='Sample size', lwd=2)
lines(ns, power.samplesize.high, col='darkmagenta', lwd=2)
abline(h = alpha, lty=2)
title("Empirical power vs. Sample Size")
```


## Two sample test of mean

Consider two sets of observations, the first set has (population) mean $\mu_1$ and the second has $\mu_2$. We are going to look at a couple of hypothesis tests for determining if the mean of each set is equal (or not). That is, the null hypothesis is given by $\mu_1 = \mu_2$.

## Test data

Generate some test data for this workshop. We use the library `mvnorm` (or `MASS`) to sample from multivariate Normal distribution.

Set the size of the two data sets equal.
```{r}
n1 = 100
n2 = 100
```

Set the dimensionality of the data.
```{r}
p = 5
```

Set the true population means of the two samples.
```{r}
mu1 = rep(0, p)
mu2 = mu1
mu2[1:3] = 0.2
```

```{r}
mu1
```

```{r}
mu2
```

We generate our standard (correlated) covariance matrix.
```{r}
rho = 0.2
Sigma = rho^(abs(outer(1:p, 1:p, "-"))) # AR1 covariance
```

```{r}
Sigma
```

Generate the two samples. We assume both sets have the same covariance.
```{r}
X = rmvn(n1, mu1, Sigma)
Y = rmvn(n2, mu2, Sigma)
```

## Implement Hotelling $T^2$ two-sample test

```{r}
hotelling.twosample = function(x, y, mu0=rep(0,ncol(x))) {
  n1 = nrow(x)
  n2 = nrow(y)
  
  p = ncol(x)

  bar.x = colMeans(x)
  bar.y = colMeans(y)

  diff.x = sweep(x, 2, bar.x)
  diff.y = sweep(y, 2, bar.y)
  
  S.pooled = 1/(n1+n2-2) * (t(diff.x) %*% diff.x + t(diff.y) %*% diff.y)
  
  z = bar.x - bar.y - mu0
  
  T2 = n1*n2/(n1+n2) * t(z) %*% solve(S.pooled) %*% z
  
  p.value = pchisq(q = T2, df = p, lower.tail = FALSE)
  
  # Alternative using F distribution
  #p.value = pf(q = (n1+n2-2)/(p*(n1+n2-2))*T2, df1=p, df2=(n1+n2-p-1), lower.tail = FALSE)
  
  return(list(stat=T2, pvalue=p.value))
}
```

We compare ours and the packages.
```{r}
hotelling.twosample(X, Y)
```

```{r}
HotellingsT2(X, Y, test = 'chi')
```


# Testing means: a high-dimensional result

## Bai and Saranadasa result 

We are now going to look at the result proposed in the paper:

*Bai ZD and Saranadasa H (1996). "Effect of high dimension: by an example of a two sample problem." Statistica Sinica, 6(2), 311–329.*

### A first go at a implementation

We reuse the previous data and obtain the number of samples in each set.
```{r}
n1 = dim(X)[1]
n2 = dim(Y)[1]
```

Calculate the parameters, etc.
```{r}
tau = (n1 * n2)/(n1 + n2)
n = n1 + n2 - 2
p = dim(X)[2]
```

Calculate the sum of square difference of means.
```{r}
diff = colMeans(X) - colMeans(Y)
```

```{r}
norm(diff, "2")^2
```

```{r}
t(diff) %*% diff
```

The pooled sample covariance, an alternative way using the in-built `cov` function.
```{r}
S = ((n1 - 1) * cov(X) + (n2 - 1) * cov(Y))/n
```

```{r}
tr = function(x) { sum(diag(x)) }
```

Calculate the proposed test statistic.
```{r}
trS = tr(S)
Bn.sq = (n^2)/((n + 2) * (n - 1)) * (tr(S %*% S) - trS^2/n)
test.BS = (tau*norm(diff, "2")^2 - trS)/sqrt( 2 * (n + 1)/n * Bn.sq )
test.BS
```

By Eq. (4.5) in the their paper, this is $N(0,1)$ so we use this information to get the $p$-value.
```{r}
pnorm(test.BS, lower.tail = FALSE)
```


### Implementation

We wrap all the previous steps into a function.

```{r}
bs.twosample = function(x, y, mu0=rep(0,ncol(x))) {
  n1 = nrow(x)
  n2 = nrow(y)
  
  p = ncol(x)
  
  tau = (n1 * n2)/(n1 + n2)
  n = n1 + n2 - 2
  
  diff = colMeans(X) - colMeans(Y)

  S = ((n1 - 1) * cov(X) + (n2 - 1) * cov(Y))/n
  trS = tr(S)
  
  Bn.sq = (n^2)/((n + 2) * (n - 1)) * (tr(S %*% S) - trS^2/n)
  test.BS = (tau*t(diff)%*%diff - trS)/sqrt( 2 * (n + 1)/n * Bn.sq )
  
  p.value = pnorm(test.BS, lower.tail = FALSE)
  
  return(list(stat=test.BS, pvalue=p.value))
}
```

We test our function on the test sample data.
```{r}
bs.twosample(X, Y)
```
### Comparison to another implementation

The Bai and Saranadasa test is available in the `highmean` package. Install it with `install.packages('highmean')`

```{r}
library(highmean)
```

We compare our result to theirs.
```{r}
apval_Bai1996(X, Y)
```

### Power

We can calculate the power of the test.

```{r}
M = 1000 # MC replicates

n = 100
p = 10
rho = 0.1
Sigma = rho^abs(outer(1:p, 1:p, "-")) # AR1 covariance

alpha = 0.05

mu0 = rep(5, p)
mu1s = seq(4.95, 5.4, length.out = 20) # alternatives
m = length(mu1s)

power = numeric(m)
for (i in 1:m) {
  p.values = numeric(M)
  for (j in 1:M) {
    mu1 = rep(mu1s[i], p)
    X = rmvn(n, mu0, Sigma)
    Y = rmvn(n, mu1, Sigma)
    p.values[j] = bs.twosample(X, Y)$pvalue
  }
  power[i] = mean(p.values <= alpha)
}

power.bs = power
```

We plot the empirical power of the Bai and Saranadasa test compared to Hotelling's $T^2$ test.

```{r}
plot(mu1s, power.bs, ylim=c(0,1), type='l', col='darkgreen', ylab='Power', xlab='mu1', lwd=2)
lines(mu1s, power.high, col='darkmagenta', lwd=2)
abline(v = mu0, lty=2)
abline(h = alpha, lty=2)
title("Empirical power")
```


## Chen and Qin (2010)

We now look at Chen and Qin's test statistic.

### Test data

Generate some test data for this workshop.

Set the size of the two data sets equal.
```{r}
n1 = 100
n2 = 100
```

Set the dimensionality of the data.
```{r}
p = 5
```

Set the true population means of the two samples.
```{r}
mu1 = rep(0, p)
mu2 = mu1
mu2[1:3] = 0.2
```

```{r}
mu1
```

```{r}
mu2
```

We use a fancy technique to generate our standard (correlated) covariance matrix.
```{r}
rho = 0.2
Sigma = rho^(abs(outer(1:p, 1:p, "-"))) # AR1 covariance
```

Generate the two samples. We assume both sets have the same covariance.
```{r}
X = rmvnorm(n1, mu1, Sigma)
Y = rmvnorm(n2, mu2, Sigma)
```


### Implement result

Get the sample sizes.
```{r}
n1 = dim(X)[1]
n2 = dim(Y)[1]
```

Dimensionality and degrees of freedom.
```{r}
p = dim(X)[2]
n = n1 + n2 - 2
```

Calculate the test statistic.
```{r}
S = ((n1 - 1)*cov(X) + (n2 - 1)*cov(Y))/n
trS = sum(diag(S))
tr.cov2 = n^2/((n + 2)*(n - 1))*(sum(t(S) %*% S) - trS^2/n)
T1 = X %*% t(X)
T2 = Y %*% t(Y)
P1 = (sum(T1) - sum(diag(T1)))/(n1*(n1 - 1))
P2 = (sum(T2) - sum(diag(T2)))/(n2*(n2 - 1))
P3 = -2*sum(X %*% t(Y))/(n1*n2)
T = P1 + P2 + P3
test.stat = as.numeric(T/sqrt((2/(n1*(n1 - 1)) + 2/(n2*(n2 - 1)) + 4/(n1*n2))*tr.cov2))
test.stat
```

From Theorem 1, the asymptotic test statistic is $N(0,1)$. We use this to get the $p$-value.
```{r}
pval = 1-pnorm(test.stat)
print(pval)
```

### Compare against different implementation

We compare our result and the implementation in the package `highmean`.

```{r}
apval_Chen2010(X, Y, eq.cov = TRUE)
```

### Power

We check the power of the test.

```{r}
M = 1000 # MC replicates

n = 100
p = 10
rho = 0.1
Sigma = rho^abs(outer(1:p, 1:p, "-")) # AR1 covariance

alpha = 0.05

mu0 = rep(5, p)
mu1s = seq(4.95, 5.4, length.out = 20) # alternatives
m = length(mu1s)

power = numeric(m)
for (i in 1:m) {
  p.values = numeric(M)
  for (j in 1:M) {
    mu1 = rep(mu1s[i], p)
    X = rmvn(n, mu0, Sigma)
    Y = rmvn(n, mu1, Sigma)
    p.values[j] = apval_Chen2010(X, Y, eq.cov = TRUE)$pval
  }
  power[i] = mean(p.values <= alpha)
}

power.cq = power

plot(mu1s, power.bs, ylim=c(0,1), type='l', col='darkgreen', ylab='Power', xlab='mu1')
lines(mu1s, power.high, col='darkmagenta')
lines(mu1s, power.cq, col='darkblue')
abline(v = mu0, lty=2)
abline(h = alpha, lty=2)
title("Empirical power")
```

```{r}
M = 100 # MC replicates

n = 10
p = 500
rho = 0.1
Sigma = rho^abs(outer(1:p, 1:p, "-")) # AR1 covariance

alpha = 0.05

mu0 = rep(5, p)
mu1s = seq(4.95, 5.4, length.out = 20) # alternatives
m = length(mu1s)

power = numeric(m)
for (i in 1:m) {
  p.values = numeric(M)
  for (j in 1:M) {
    # mu1 = rep(0, p)
    # mu1[1:10] = mu1s[i]
    mu1 = rep(mu1s[i], p)
    X = rmvn(n, mu0, Sigma)
    Y = rmvn(n, mu1, Sigma)
    p.values[j] = apval_Chen2010(X, Y, eq.cov = TRUE)$pval
  }
  power[i] = mean(p.values <= alpha)
}

power.cq = power

plot(mu1s, power.cq, ylim=c(0,1), type='l', col='darkblue', ylab='Power', xlab='mu1')
abline(v = mu0, lty=2)
abline(h = alpha, lty=2)
title("Empirical power")
```

### Different population covariances

Chen and Qin (2010) also proposes a test when the two sets of observations have different population covariances, see p.814. This could be implemented as follows:

```{r}
n1 = dim(X)[1]
n2 = dim(Y)[1]
p = dim(X)[2]
T1 = X %*% t(X)
T2 = Y %*% t(Y)
P1 = (sum(T1) - sum(diag(T1)))/(n1*(n1 - 1))
P2 = (sum(T2) - sum(diag(T2)))/(n2*(n2 - 1))
P3 = -2*sum(X %*% t(Y))/(n1*n2)
T = P1 + P2 + P3

tr.cov1.sq = tr.cov2.sq = tr.cov1.cov2 = 0
for(j in 1:n1){
	for(k in 1:n1){
		if(j != k){
			tempmean = (colSums(X) - X[j,] - X[k,])/(n1 - 2)
			P1 = sum(X[j,]*(X[k,] - tempmean))
			P2 = sum(X[k,]*(X[j,] - tempmean))
			tr.cov1.sq = tr.cov1.sq + P1*P2
		}
	}
}

tr.cov1.sq = tr.cov1.sq/(n1*(n1 - 1))
for(j in 1:n2){
	for(k in 1:n2){
		if(j != k){
			tempmean = (colSums(Y) - Y[j,] - Y[k,])/(n2 - 2)
			P1 = sum(Y[j,]*(Y[k,] - tempmean))
			P2 = sum(Y[k,]*(Y[j,] - tempmean))
			tr.cov2.sq = tr.cov2.sq + P1*P2
		}
	}
}

tr.cov2.sq = tr.cov2.sq/(n2*(n2 - 1))
for(j in 1:n1){
	for(k in 1:n2){
		tempmean1 = (colSums(X) - X[j,])/(n1 - 1)
		tempmean2 = (colSums(Y) - Y[k,])/(n2 - 1)
		P1 = sum(X[j,]*(Y[k,] - tempmean2))
		P2 = sum(Y[k,]*(X[j,] - tempmean1))
		tr.cov1.cov2 = tr.cov1.cov2 + P1*P2
	}
}
tr.cov1.cov2 = tr.cov1.cov2/(n1*n2)

test.stat = T/sqrt(2/(n1*(n1 - 1))*tr.cov1.sq + 2/(n2*(n2 - 1))*tr.cov2.sq + 4/(n1*n2)*tr.cov1.cov2)
test.stat = as.numeric(test.stat)
pval = 1 - pnorm(test.stat)
print(pval)
```

# Blueprint: Classic -> MP -> UHD

```{r}
p = 50
n1 = 100
n2 = 100
n = n1 + n2 - 2
tau = (n1 * n2)/(n1 + n2)

mu0 = rep(0, p) # difference
mu1 = rep(0, p)
mu2 = mu1
mu2[1:3] = 0.2

#Sigma = 0.2^(abs(outer(1:p, 1:p, "-")))
Sigma = diag(p)

nsims = 1000
test.stat = replicate(nsims, {
  X = rmvn(n1, mu1, Sigma)
  Y = rmvn(n2, mu2, Sigma)
  
  bar.x = colMeans(X)
  bar.y = colMeans(Y)
  
  S = ((n1 - 1)*cov(X) + (n2 - 1)*cov(Y))/n
  z = bar.x - bar.y - mu0
  trS = sum(diag(S))
  
  T.Hotelling = tau * t(z) %*% solve(S) %*% z

  Bn.sq = (n^2)/((n + 2) * (n - 1)) * (tr(S %*% S) - trS^2/n)
  T.BS = (tau * t(z) %*% z - trS) / sqrt( 2 * (n + 1)/n * Bn.sq )

  T1 = X %*% t(X)
  T2 = Y %*% t(Y)
  
  P1 = (sum(T1) - sum(diag(T1)))/(n1*(n1 - 1))
  P2 = (sum(T2) - sum(diag(T2)))/(n2*(n2 - 1))
  P3 = -2*sum(X %*% t(Y))/(n1*n2)
  T.ChenQin = (P1 + P2 + P3) / sqrt((2/(n1*(n1 - 1)) + 2/(n2*(n2 - 1)) + 4/(n1*n2))*tr.cov2)
  
  c(T.Hotelling, T.BS, T.ChenQin)
})

breaks = seq(1.2 * min(test.stat), 1.2*max(test.stat), length.out=50)
hist(test.stat[1,], breaks, plot=FALSE) -> h1
hist(test.stat[2,], breaks, plot=FALSE) -> h2
hist(test.stat[3,], breaks, plot=FALSE) -> h3
plot(h1$mids, h1$density, type="S", col="darkred", ylim=c(0, 0.5), xlab="", ylab="")
lines(h2$mids, h2$density, type="S", col="red")
lines(h3$mids, h3$density, type="S", col="magenta")
legend("topright", c("Hotelling", "BaiSarandasa", "ChenQin"), fill=c("darkred", "red", "magenta"))
```
```{r}
length(h1$density)
```


# Change-point detection

## One-dimensional case

We are first going to generate some test data.

```{r}
x = rnorm(300) + c(rep(2,50), rep(0,250))
```

```{r}
plot(x)
```

`install.packages('wbs')`

```{r}
library(wbs)
```

```{r}
s = sbs(x)
```

```{r}
s.cpt = changepoints(s)
s.cpt
```

See this review of R packages: https://lindeloev.github.io/mcp/articles/packages.html
